<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="https://dontfollowmeimcrazy.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dontfollowmeimcrazy.github.io/" rel="alternate" type="text/html" /><updated>2020-08-27T21:32:53+02:00</updated><id>https://dontfollowmeimcrazy.github.io/feed.xml</id><title type="html">orso.io</title><subtitle>Hello! This is the den of an atypical bear named Matteo Dunnhofer. </subtitle><entry><title type="html">New paper at ECCV 2020</title><link href="https://dontfollowmeimcrazy.github.io/2020/08/27/new-paper-at-eccv-2020/" rel="alternate" type="text/html" title="New paper at ECCV 2020" /><published>2020-08-27T00:00:00+02:00</published><updated>2020-08-27T00:00:00+02:00</updated><id>https://dontfollowmeimcrazy.github.io/2020/08/27/new-paper-at-eccv-2020</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2020/08/27/new-paper-at-eccv-2020/">&lt;p&gt;Our newest paper &lt;strong&gt;“An Exploration of Target-Conditioned Segmentation Methods for Visual Object Trackers”&lt;/strong&gt; is going to be presented on August 28th at the &lt;a href=&quot;https://www.votchallenge.net/vot2020/&quot;&gt;Visual Object Tracking Challenge VOT2020&lt;/a&gt; workshop, held in conjunction with the &lt;a href=&quot;https://eccv2020.eu&quot;&gt;European Conference on Computer Vision (ECCV) 2020&lt;/a&gt;. In this work, we propse an analysis of the most recent segmentation methods that are conditioned on a particular object, in order to transform any bounding-box tracker into a segmentation tracker.&lt;/p&gt;

&lt;p&gt;Here is the abstract:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Visual object tracking is the problem of predicting a target object’s state in a video. Generally, bounding-boxes have been used to represent states, and a surge of effort has been spent by the community to produce efficient causal algorithms capable of locating targets with such representations. As the field is moving towards binary segmentation masks to define objects more precisely, in this paper we propose to extensively explore target-conditioned segmentation methods available in the computer vision community, in order to transform any bounding-box tracker into a segmentation tracker. Our analysis shows that such methods allow trackers to compete with recently proposed segmentation trackers, while performing quasi real-time.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;and in the following you can have a look to a teaser video of our study (follow the links in the description for a longer presentation and qualitative examples).&lt;/p&gt;

&lt;div class=&quot;post-video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/watch?v=Gcmiv4s1J8w&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="deep learning" /><category term="visual tracking" /><category term="vot2020" /><category term="eccv2020" /><category term="uniud" /><summary type="html">Our newest paper “An Exploration of Target-Conditioned Segmentation Methods for Visual Object Trackers” is going to be presented on August 28th at the Visual Object Tracking Challenge VOT2020 workshop, held in conjunction with the European Conference on Computer Vision (ECCV) 2020. In this work, we propse an analysis of the most recent segmentation methods that are conditioned on a particular object, in order to transform any bounding-box tracker into a segmentation tracker.</summary></entry><entry><title type="html">First journal paper published</title><link href="https://dontfollowmeimcrazy.github.io/2020/01/15/first-journal-paper-published/" rel="alternate" type="text/html" title="First journal paper published" /><published>2020-01-15T13:26:26+01:00</published><updated>2020-01-15T13:26:26+01:00</updated><id>https://dontfollowmeimcrazy.github.io/2020/01/15/first-journal-paper-published</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2020/01/15/first-journal-paper-published/">&lt;p&gt;I am happy to tell you that my first journal paper &lt;strong&gt;“Siam-U-Net: encoder-decoder siamese network for knee cartilage tracking in ultrasound images”&lt;/strong&gt; is going to be published in the volume 60 of &lt;a href=&quot;https://www.journals.elsevier.com/medical-image-analysis&quot;&gt;Medical Image Analysis&lt;/a&gt;. You can find the manuscript at &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S1361841519301677&quot;&gt;this link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here is the abstract of the paper:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The tracking of the knee femoral condyle cartilage during ultrasound-guided minimally invasive procedures is important to avoid damaging this structure during such interventions. In this study, we propose a new deep learning method to track, accurately and efficiently, the femoral condyle cartilage in ultrasound sequences, which were acquired under several clinical conditions, mimicking realistic surgical setups. Our solution, that we name Siam-U-Net, requires minimal user initialization and combines a deep learning segmentation method with a siamese framework for tracking the cartilage in temporal and spatio-temporal sequences of 2D ultrasound images. Through extensive performance validation given by the Dice Similarity Coefficient, we demonstrate that our algorithm is able to track the femoral condyle cartilage with an accuracy which is comparable to experienced surgeons. It is additionally shown that the proposed method outperforms state-of-the-art segmentation models and trackers in the localization of the cartilage. We claim that the proposed solution has the potential for ultrasound guidance in minimally invasive knee procedures.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;and in the following video you can have a look to a short recap of our study.&lt;/p&gt;

&lt;div class=&quot;post-video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/nYTlEyMoj0s&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="deep learning" /><category term="knee cartilage" /><category term="medical image analysis" /><category term="siam-u-net" /><category term="tracking" /><category term="ultrasound" /><category term="uniud" /><category term="qut" /><summary type="html">I am happy to tell you that my first journal paper “Siam-U-Net: encoder-decoder siamese network for knee cartilage tracking in ultrasound images” is going to be published in the volume 60 of Medical Image Analysis. You can find the manuscript at this link.</summary></entry><entry><title type="html">First paper accepted at the VOT2019 Challenge Workshop</title><link href="https://dontfollowmeimcrazy.github.io/2019/09/27/first-paper-accepted-at-the-vot2019-challenge-workshop/" rel="alternate" type="text/html" title="First paper accepted at the VOT2019 Challenge Workshop" /><published>2019-09-27T18:31:44+02:00</published><updated>2019-09-27T18:31:44+02:00</updated><id>https://dontfollowmeimcrazy.github.io/2019/09/27/first-paper-accepted-at-the-vot2019-challenge-workshop</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2019/09/27/first-paper-accepted-at-the-vot2019-challenge-workshop/">&lt;p&gt;I am excited to announce that my very first first-author paper titled &lt;strong&gt;“Visual Tracking by means of Deep Reinforcement Learning and an Expert Demonstrator”&lt;/strong&gt; was accepted at the &lt;a href=&quot;http://www.votchallenge.net/vot2019/index.html&quot;&gt;Visual Object Tracking (VOT) 2019 Challenge&lt;/a&gt;. This workshop is the annual premier event in the visual tracking panorama and this year it will be held in conjunction with the &lt;a href=&quot;http://iccv2019.thecvf.com&quot;&gt;International Conference on Computer Vision (ICCV) 2019&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here is the abstract of the paper:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In the last decade many different algorithms have been proposed to track a generic object in videos. Their execution on recent large-scale video datasets can produce a great amount of various tracking behaviours. New trends in Reinforcement Learning showed that demonstrations of an expert agent can be efficiently used to speed-up the process of policy learning. Taking inspiration from such works and from the recent applications of Reinforcement Learning to visual tracking, we propose two novel trackers, A3CT, which exploits demonstrations of a state-of-the-art tracker to learn an effective tracking policy, and A3CTD, that takes advantage of the same expert tracker to correct its behaviour during tracking. Through an extensive experimental validation on the GOT-10k, OTB-100, LaSOT, UAV123 and VOT benchmarks, we show that the proposed trackers achieve state-of-the-art performance while running in real-time.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The preprint can be found on &lt;a href=&quot;https://arxiv.org/abs/1909.08487&quot;&gt;arXiv&lt;/a&gt;. Here below you can have a look to some videos where we show the performance of our proposed trackers.&lt;/p&gt;

&lt;div class=&quot;post-video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/jSGLafk4-G4&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="a3ct" /><category term="a3ctd" /><category term="deep learning" /><category term="deep reinforcement learning" /><category term="iccv" /><category term="visual tracking" /><category term="vot" /><summary type="html">I am excited to announce that my very first first-author paper titled “Visual Tracking by means of Deep Reinforcement Learning and an Expert Demonstrator” was accepted at the Visual Object Tracking (VOT) 2019 Challenge. This workshop is the annual premier event in the visual tracking panorama and this year it will be held in conjunction with the International Conference on Computer Vision (ICCV) 2019.</summary></entry><entry><title type="html">bandidos en barcelona</title><link href="https://dontfollowmeimcrazy.github.io/2017/11/29/bandidos-en-barcelona/" rel="alternate" type="text/html" title="bandidos en barcelona" /><published>2017-11-29T22:38:02+01:00</published><updated>2017-11-29T22:38:02+01:00</updated><id>https://dontfollowmeimcrazy.github.io/2017/11/29/bandidos-en-barcelona</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2017/11/29/bandidos-en-barcelona/">&lt;p&gt;I finally had the time to edit the clips that my friends and I filmed during our skate-trip to Barcelona two months ago. Do not expect big tricks, we wanted just to have fun cruising the around the streets. We had very good times, although the Catalonian crisis, people was still really friendly and smiling. Barcelona is really an amazing city!&lt;/p&gt;

&lt;div class=&quot;post-video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/2C-LwgQEsgw&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><author><name>dontfollowmeimcrazy</name></author><summary type="html">I finally had the time to edit the clips that my friends and I filmed during our skate-trip to Barcelona two months ago. Do not expect big tricks, we wanted just to have fun cruising the around the streets. We had very good times, although the Catalonian crisis, people was still really friendly and smiling. Barcelona is really an amazing city!</summary></entry><entry><title type="html">2007-2017</title><link href="https://dontfollowmeimcrazy.github.io/2017/10/02/2007-2017/" rel="alternate" type="text/html" title="2007-2017" /><published>2017-10-02T14:41:32+02:00</published><updated>2017-10-02T14:41:32+02:00</updated><id>https://dontfollowmeimcrazy.github.io/2017/10/02/2007-2017</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2017/10/02/2007-2017/">&lt;p&gt;This year my crew and I celebrate our first ten years on the skateboard. For the occasion we decided to produce some skateboard decks and so I had to take back the pencil and start drawing (like in the good old days). In the drawing it is represented, in a certain way, our “skateboarding path” from where we began to where we are now. You can see the result above, and the final printed board below. We are very happy with them.&lt;/p&gt;

&lt;p&gt;Now we go skate!&lt;/p&gt;

&lt;div class=&quot;post-image&quot;&gt;
  &lt;img class=&quot;post-image&quot; src=&quot;https://dontfollowmeimcrazy.github.io/assets/images/blog/2017-10-02/v1_cropgg.jpg&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image&quot;&gt;
  &lt;img class=&quot;post-image&quot; src=&quot;https://dontfollowmeimcrazy.github.io/assets/images/blog/2017-10-02/IMG_3736.jpg&quot; /&gt;
&lt;/div&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="10 years" /><category term="pine" /><category term="skate" /><category term="skateboarding" /><category term="tarvisio" /><summary type="html">This year my crew and I celebrate our first ten years on the skateboard. For the occasion we decided to produce some skateboard decks and so I had to take back the pencil and start drawing (like in the good old days). In the drawing it is represented, in a certain way, our “skateboarding path” from where we began to where we are now. You can see the result above, and the final printed board below. We are very happy with them.</summary></entry><entry><title type="html">My first Kaggle competition</title><link href="https://dontfollowmeimcrazy.github.io/2017/08/04/my-first-kaggle-competition/" rel="alternate" type="text/html" title="My first Kaggle competition" /><published>2017-08-05T00:08:44+02:00</published><updated>2017-08-05T00:08:44+02:00</updated><id>https://dontfollowmeimcrazy.github.io/2017/08/04/my-first-kaggle-competition</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2017/08/04/my-first-kaggle-competition/">&lt;p&gt;I knew of the existence of Kaggle since I started studying Machine Learning. Unfortunately, during that time I did not have the experience nor the time to take part to any competition. But in last semester I made some good experiences that helped me to develop my skills in the field. At the same time, an interesting real world problem was published as a &lt;a href=&quot;https://www.kaggle.com/c/planet-understanding-the-amazon-from-space&quot;&gt;competition on the  Kaggle platform&lt;/a&gt;. And so I realised that that was the best one to give myself a challenge.&lt;/p&gt;

&lt;div class=&quot;post-image&quot;&gt;
  &lt;img class=&quot;post-image&quot; src=&quot;https://dontfollowmeimcrazy.github.io/assets/images/blog/2017-08-04/habitation1.jpg&quot; /&gt;
  
  &lt;p class=&quot;caption&quot;&gt;
    An example of satellite image used in the competition. Credits: Planet and Kaggle.
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://www.planet.com&quot;&gt;Planet Labs&lt;/a&gt; is a private company that owns the multitude of imaging-satellites that orbit around the Earth. These satellites form a constellation that can provide a complete image of our planet, all the time. This great amount of data is used by governments and  humanitarian, business, environmental (and many more) organisations to monitor the state and the changes of Earth’s ground. These stakeholders can also use the images to study the evolution or track the variations in different (maybe critic) areas of the planet, and exactly that is the context of the competition organised by Planet and its Brazilian partner SCCON: the deforestation of the Amazon rainforest. The seriousness of this problem (and it is really serious!) obviously grabs the attention of many researchers that use satellite data to track the rapid changes that occur inside the forest. But often its the huge vastness make the process of analysing the images really tedious, and a system that could automatically identify deforestation risk zones will certainly benefit the work of these people. These kind of procedures already exist but they do not for the images produced by Planet nor at an high level of resolution. So, Planet decided to challenge Kaggle’s users with the goal to develop a software tool able to classify the atmospheric condition and ground characteristics contained inside satellite images of the Amazon.&lt;/p&gt;

&lt;p&gt;The dataset provided consisted of more than 40000 training examples. Every example (a chip image) was a 256px x 256px GeoTiff 4-band (red, green, blue, near infrared) image that covered an area of there forest of circa 90 hectares (950mt x 950mt circa). The associated labels explicated the weather condition (clear, cloudy, partly cloudy, haze) and the characteristics of the ground in that particular area (primary rainforest, water, habitation, agricolture, cultivation, road, bare ground, slash-and-burn cultivation, selective logging, blooming trees, conventional mines, artisanal mines and blow down). Every sample was classified with exactly one weather label and at least one (at most all) label for the ground. So, this happened to be a multi-label multi-class classification problem, instead of a classic multi-class classification. The labels were very unbalanced, making some classes really difficult to predict. Moreover, together with the GeoTiff images, Planet provided their RGB version for practice. Finally, the test set contained 61191 samples which labels had to be predicted. The metric used by Kaggle for the evaluation was the F2 score.&lt;/p&gt;

&lt;p&gt;Given this good amount of training data, the use of Convolutional Neural Network (today the standard approach in Computer Vision) is very natural. To produce my solution I implemented different models of CNN, and &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;ResNet&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1608.06993&quot;&gt;DenseNet&lt;/a&gt; (the latest ImageNet’s competition winners) were the ones that performed best. I trained both from-scratch and pretrained nets, the latter giving some improvements in the score. Although ImageNet does not contain this kind of satellite images, the features produced by pretrained models were more meaningful. In all the models, the last activation (usually a softmax) was replaced with a sigmoid. This was done to compute every class probability independently from the other classes, and so I could use labels as one-hot vectors with many ones as the number of classes present in the image. Classic tricks like data augmentation and normalisation, learning rate scheduling etc were performed.&lt;/p&gt;

&lt;p&gt;The aim of Planet was to obtain models built on the GeoTiff images. Turned out from the experiments that the RGB images (provided only for practice) were more significant for the nets, and models trained on them performed definitely better than the one trained on the 4-band images. This was caused by a lot misclassifications in the latter.&lt;/p&gt;

&lt;p&gt;For the predictions on the test set, instead of using the output of the single models, I implemented an ensemble of the best performing nets (that I evaluated on a validation set produced splitting the training set). In particular, I made ResNet34, ResNet101, DenseNet121, DenseNet169 and DenseNet201 vote for the labels of every test example. Then, classes that obtained a vote greater of equal to half of the number of models (in this case if a label was voted by at least three models) were retained. Before that, I also optimised the threshold of the class probabilities (through a brute force search), meaning that a class was predicted if its probability was greater than a particular value.&lt;/p&gt;

&lt;p&gt;All the sources were developed in Python. For the models trained from scratch I used TensorFlow, to use the pretrained nets I employed PyTorch.&lt;/p&gt;

&lt;p&gt;My solution obtained a final F2 score of 0.92942 that earned me the 88th position (inside the top 10%) and the bronze medal.&lt;/p&gt;

&lt;p&gt;In conclusion, I am very happy for this first result and it has been a great and exciting experience from which a learned a lot of things. Among the others, it allowed me to study and implement complex and very effective CNNs, to face real world datasets (and they are not as the notorious you find in papers), to learn how ensembling can give a boost to prediction, and to discover the flexibility and simplicity of PyTorch.&lt;/p&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="Amazon" /><category term="deep learning" /><category term="deforestation" /><category term="Kaggle" /><category term="machine learning" /><category term="rainforest" /><summary type="html">I knew of the existence of Kaggle since I started studying Machine Learning. Unfortunately, during that time I did not have the experience nor the time to take part to any competition. But in last semester I made some good experiences that helped me to develop my skills in the field. At the same time, an interesting real world problem was published as a competition on the  Kaggle platform. And so I realised that that was the best one to give myself a challenge.</summary></entry><entry><title type="html">ASD Forest Park Crew membership registration</title><link href="https://dontfollowmeimcrazy.github.io/2017/06/20/asd-forest-park-crew-membership-registration/" rel="alternate" type="text/html" title="ASD Forest Park Crew membership registration" /><published>2017-06-20T13:54:49+02:00</published><updated>2017-06-20T13:54:49+02:00</updated><id>https://dontfollowmeimcrazy.github.io/2017/06/20/asd-forest-park-crew-membership-registration</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2017/06/20/asd-forest-park-crew-membership-registration/">&lt;p&gt;Hello everyone!&lt;/p&gt;

&lt;p&gt;During last spring I had the opportunity to do a little work for the ASD Forest Park Crew association (of which I am a member) that runs my hometown skatepark.&lt;/p&gt;

&lt;p&gt;To access our skatepark a person must be a member of our association. Until last year, to become a fellow a person had to present himself at the skatepark, pay the registration fee and then get a paper card valid for the access to the park. A classic registration scenario. Starting from this season we decided to automate and digitize all this process. So, I realized a little web-app to manage the registration, the payment via PayPal and the generation of a PDF file that counts as a valid pass for the skatepark. With this “innovation” we are able to manage electronically and efficiently all the data about the association’s fellows and to reduce some managing costs.&lt;/p&gt;

&lt;p&gt;You can check the app at this &lt;a href=&quot;http://orso.io/forestpark/registration&quot;&gt;page&lt;/a&gt;. For every kind of info or to report a malfunction please send me an E-Mail.&lt;/p&gt;

&lt;p&gt;We’ll wait for you at the skatepark 😉.&lt;/p&gt;

&lt;div class=&quot;post-image&quot;&gt;
  &lt;img class=&quot;post-image&quot; src=&quot;https://dontfollowmeimcrazy.github.io/assets/images/blog/2017-06-20/park.jpg&quot; /&gt;
  
  &lt;p class=&quot;caption&quot;&gt;
    Forest Park skatepark in Tarvisio.
  &lt;/p&gt;
&lt;/div&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="Forest Park" /><category term="registration" /><category term="Skateboard" /><category term="skatepark" /><category term="tarvisio" /><summary type="html">Hello everyone!</summary></entry><entry><title type="html">A first look to ImageNet</title><link href="https://dontfollowmeimcrazy.github.io/2017/02/02/a-first-look-to-imagenet/" rel="alternate" type="text/html" title="A first look to ImageNet" /><published>2017-02-02T11:54:05+01:00</published><updated>2017-02-02T11:54:05+01:00</updated><id>https://dontfollowmeimcrazy.github.io/2017/02/02/a-first-look-to-imagenet</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2017/02/02/a-first-look-to-imagenet/">&lt;p&gt;In the last years a lot of coverage was given to Artificial Intelligence. During the last months of my Bachelor’s, reading more and more about the successful applications, I started to feel really curious about that area of AI called &lt;strong&gt;Deep Learning&lt;/strong&gt;. Actually, Deep Learning is a sub-field of the more general &lt;strong&gt;Machine Learning&lt;/strong&gt;, that is the field of Computer Science which “gives a computer the ability to learn without be explicitly programmed” (&lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;Wikipedia&lt;/a&gt;) or, in different words, aims to design programs that can improve their performance by “learning” from examples (a better and easier definition &lt;a href=&quot;https://www.quora.com/What-is-the-best-description-you-can-give-to-the-subject-of-machine-learning-to-a-layperson-or-a-kid&quot;&gt;here&lt;/a&gt;). The theories of Deep Learning focus on building Deep Neural Networks (DNN), which are complex mathematical models that loosely mimic the behaviour of the human brain. The procedure to make DNNs “intelligent”; simulates the way people (and especially babies) learn: you perceive something, you make sense of it and “change your mind” accordingly. Really fascinating!&lt;/p&gt;

&lt;div class=&quot;post-image&quot;&gt;
  &lt;img class=&quot;post-image&quot; src=&quot;https://dontfollowmeimcrazy.github.io/assets/images/blog/2017-02-02/tsne-full.jpg&quot; /&gt;
  
  &lt;p class=&quot;caption&quot;&gt;
    Samples of similar images that were used to train AlexNet. They are arranged by the similarity of the abstractions they contain.
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Luckily, my university offers a Machine Learning course so I had the opportunity to study and put hands on the things I was interested. As part of a project, I was asked to implement and train a &lt;strong&gt;Convolutional Neural Network&lt;/strong&gt; (ConvNet), a special type of neural network that is usually applied to images (they are used in your Prisma filters too!). You can think this kind of networks as the artificial counterpart of our vision system. ConvNets are composed of different layers of artificial neurons that are able to learn various levels of abstractions: for example, when applied to an image of a person, the first layers of neurons get excited when they recognise simple patterns as lines or edges; the immediate following layers use the edges to compose more complicated abstractions such as eyes, nose, mouth, hands, etc; then towards the last layers, the previous patters are merged in a full abstraction of the person. To learn this “internal representation” these models are shown just a lot if images (with their category associated) a lot of times. Today, ConvNets are the standard in Computer Vision.&lt;/p&gt;

&lt;p&gt;I implemented &lt;strong&gt;&lt;a href=&quot;http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf&quot;&gt;AlexNet&lt;/a&gt;&lt;/strong&gt;, the first model of ConvNet that was used by the scientific community to win the ILSVRC competition. I trained it showing and showing a set of &lt;em&gt;1.2 million images&lt;/em&gt; of &lt;em&gt;1000 different categories&lt;/em&gt; (the famous &lt;strong&gt;&lt;a href=&quot;http://image-net.org/challenges/LSVRC/2012/&quot;&gt;ImageNet&lt;/a&gt;&lt;/strong&gt;dataset) for more than forty times. The training took a few days. It was stopped after the neural network reached a good level of generalisation, that is the ability to predict the category of examples that it has never seen. Once trained, if you input it with an image, it will output the top five categories that best classify the image. For example, feeding AlexNet with the image below&lt;/p&gt;

&lt;div class=&quot;post-image&quot;&gt;
  &lt;img class=&quot;post-image&quot; src=&quot;https://dontfollowmeimcrazy.github.io/assets/images/blog/2017-02-02/lussari.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;gives the output:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;AlexNet saw:
alp - score: 0.575796604156
church, church building - score: 0.0516746938229
valley, vale - score: 0.0432425364852
castle - score: 0.0284509658813
monastery - score: 0.0265731271356
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where the categories are ordered by the probability (score) that the image belongs to that class.&lt;/p&gt;

&lt;p&gt;I created a &lt;a href=&quot;https://github.com/dontfollowmeimcrazy/imagenet&quot;&gt;repository on GitHub&lt;/a&gt; with all the source codes I used to implement, train and test AlexNet. I coded them in Python using Google’s &lt;a href=&quot;https://www.tensorflow.org&quot;&gt;TensorFlow&lt;/a&gt; framework. If you like to dive &lt;em&gt;deeper&lt;/em&gt; into the technical details, refer to the repo.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EDIT 20-11-2017:&lt;/strong&gt; I added to the repository a version of the code using TensorFlow’s new imperative style, &lt;a href=&quot;https://research.googleblog.com/2017/10/eager-execution-imperative-define-by.html&quot;&gt;TensorFlow Eager&lt;/a&gt;. Unfortunately I did not have the time nor the resources to train and test AlexNet using these new scripts, but I expect results similar to the old ones (model architecture and training pipeline remained the same). If anyone would try to take charge of this, it would be really appreciated!&lt;/p&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="AlexNet" /><category term="ConvNet" /><category term="Deep Learning" /><category term="ImageNet" /><category term="Machine Learning" /><category term="TensorFlow" /><summary type="html">In the last years a lot of coverage was given to Artificial Intelligence. During the last months of my Bachelor’s, reading more and more about the successful applications, I started to feel really curious about that area of AI called Deep Learning. Actually, Deep Learning is a sub-field of the more general Machine Learning, that is the field of Computer Science which “gives a computer the ability to learn without be explicitly programmed” (Wikipedia) or, in different words, aims to design programs that can improve their performance by “learning” from examples (a better and easier definition here). The theories of Deep Learning focus on building Deep Neural Networks (DNN), which are complex mathematical models that loosely mimic the behaviour of the human brain. The procedure to make DNNs “intelligent”; simulates the way people (and especially babies) learn: you perceive something, you make sense of it and “change your mind” accordingly. Really fascinating!</summary></entry><entry><title type="html">The Pine video</title><link href="https://dontfollowmeimcrazy.github.io/2016/12/08/the-pine-video/" rel="alternate" type="text/html" title="The Pine video" /><published>2016-12-08T18:11:44+01:00</published><updated>2016-12-08T18:11:44+01:00</updated><id>https://dontfollowmeimcrazy.github.io/2016/12/08/the-pine-video</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2016/12/08/the-pine-video/">&lt;p&gt;On this day, two years ago, the first Pine crew’s video was published on YouTube.&lt;/p&gt;

&lt;p&gt;It featured parts of Marco Sandrini, Luca Minigher, Johnny Fabiani and me.&lt;/p&gt;

&lt;p&gt;Check it!&lt;/p&gt;

&lt;div class=&quot;post-video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/OPcLZYuM6yg&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="Friuli" /><category term="Skateboard" /><category term="tarvisio" /><category term="Udine" /><summary type="html">On this day, two years ago, the first Pine crew’s video was published on YouTube.</summary></entry><entry><title type="html">FasterApp iOS version 1.3</title><link href="https://dontfollowmeimcrazy.github.io/2016/11/08/fasterapp-ios-version-1-3/" rel="alternate" type="text/html" title="FasterApp iOS version 1.3" /><published>2016-11-08T17:36:36+01:00</published><updated>2016-11-08T17:36:36+01:00</updated><id>https://dontfollowmeimcrazy.github.io/2016/11/08/fasterapp-ios-version-1-3</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2016/11/08/fasterapp-ios-version-1-3/">&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;I finally found the time to update my application, since iOS 10 came out. This latest version, which is 1.3, is &lt;a href=&quot;https://itunes.apple.com/it/app/fasterapp/id1024576070?mt=8&quot;&gt;already up&lt;/a&gt; on the AppStore.&lt;/p&gt;

&lt;p&gt;One of the greatest features of iOS 10 is the opening of Siri to developers. The things you can program Siri to answer to are still limited but I was willing to experiment them a little. So I managed to use Siri’s abilities to let the user start a gameplay modality just with its voice. My “programmed” phrases, that are consequentially recognised by Siri, fall in the domain of workouts and fitness. Examples are: “I want to play with FasterApp”; ”Start a Free mode workout with FasterApp”; “I want to run on Arcade mode with FasterApp”; “Let’s take a run on a Personal track with FasterApp”. I hope to get these things working better in the next updates.&lt;/p&gt;

&lt;p&gt;In addition to iOS 10 support and the integration with Siri, version 1.3 of FasterApp features a new button to change the view during the gameplay. With this, you can switch between two angles of sight on the map you are playing, allowing you to better orient yourself. Finally, I made minor improvements to the user experience adding sounds, animations and better graphics.&lt;/p&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="1.3" /><category term="FasterApp" /><category term="update" /><summary type="html">Hello!</summary></entry></feed>